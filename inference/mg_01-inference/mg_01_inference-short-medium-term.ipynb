{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MG_01 Inference - Ensemble Model - 24 Hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import mysql.connector\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "from calendar import month_abbr\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import datetime\n",
    "import tempfile\n",
    "import boto3\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 min model loaded\n",
      "30 min model loaded\n",
      "45 min model loaded\n",
      "60 min model loaded\n",
      "lr_model loaded\n",
      "rf_model loaded\n",
      "xgb_model loaded\n",
      "xgb_label_encoder loaded\n",
      "knn_model loaded\n",
      "svr_model loaded\n",
      "svr_X_standardscalar loaded\n",
      "svr_y_standardscalar loaded\n",
      "ensemble_model loaded\n"
     ]
    }
   ],
   "source": [
    "aws_access_key_id = 'AKIATAVK2UELBEVSLANM'\n",
    "aws_secret_access_key = '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'\n",
    "\n",
    "s3_client = boto3.client('s3', \n",
    "                         aws_access_key_id=aws_access_key_id, \n",
    "                         aws_secret_access_key=aws_secret_access_key)\n",
    "\n",
    "#########\n",
    "# 15 Min\n",
    "bucket_name = 'mg01-models'\n",
    "key = 'linear_model_15m.joblib'\n",
    "# read from S3 bucket\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    lr_15m = joblib.load(fp)\n",
    "print(\"15 min model loaded\")\n",
    "\n",
    "#########\n",
    "# 30 Min\n",
    "key = 'linear_model_30m.pkl'\n",
    "# read from S3 bucket\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    lr_30m = joblib.load(fp)\n",
    "print(\"30 min model loaded\")\n",
    " \n",
    "#########\n",
    "# 45 Min\n",
    "key = 'linear_model_45m.pkl'\n",
    "# read from S3 bucket\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    lr_45m = joblib.load(fp)\n",
    "print(\"45 min model loaded\")\n",
    "\n",
    "#########\n",
    "# 60 Min\n",
    "key = 'linear_model_60m.pkl'\n",
    "# read from S3 bucket\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    lr_60m = joblib.load(fp)\n",
    "print(\"60 min model loaded\")\n",
    "    \n",
    "\n",
    "#########\n",
    "# 24 Hour\n",
    "bucket_name = 'ipowermigrid.24h.models'\n",
    "key = 'linear_model_24h.joblib'\n",
    "# read model from S3 bucket\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    lr_model = joblib.load(fp)\n",
    "    print(\"lr_model loaded\")\n",
    "\n",
    "key = 'randomforest_model_24h.joblib'\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    rf_model = joblib.load(fp)\n",
    "    print(\"rf_model loaded\")\n",
    "    \n",
    "key = 'xgboost_model_24h.joblib'\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    xgb_model = joblib.load(fp)\n",
    "    print(\"xgb_model loaded\")\n",
    "    \n",
    "# xgb_label_encoder load \n",
    "key = 'xgb_label_encoder-hr-24.pkl'\n",
    "# read model from S3 bucket\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    lbl = joblib.load(fp)\n",
    "    print(\"xgb_label_encoder loaded\")\n",
    "\n",
    "key = 'knnr_model_24h.joblib'\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    knn_model = joblib.load(fp)\n",
    "    print(\"knn_model loaded\")\n",
    "    \n",
    "key = 'svr_model_24h.joblib'\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    svr_model = joblib.load(fp)\n",
    "    print(\"svr_model loaded\")\n",
    "    \n",
    "key = 'svr_X_standardscalar-hr-24.pkl'\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    sc_X = joblib.load(fp)\n",
    "    print(\"svr_X_standardscalar loaded\")\n",
    "    \n",
    "key = 'svr_y_standardscalar-hr-24.pkl'\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    sc_y = joblib.load(fp)\n",
    "    print(\"svr_y_standardscalar loaded\")\n",
    "    \n",
    "key = 'ensemble_model_24h.joblib'\n",
    "with tempfile.TemporaryFile() as fp:\n",
    "    s3_client.download_fileobj(Fileobj=fp, Bucket=bucket_name, Key=key)\n",
    "    fp.seek(0)\n",
    "    ensemble_model = joblib.load(fp)\n",
    "    print(\"ensemble_model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to sql database\n",
    "credentials = 'mysql://capstone_user:Capstone22!@capstone-database.czwmid1hzf1x.us-west-2.rds.amazonaws.com/mysqldb'\n",
    "\n",
    "mydb = mysql.connector.connect(\n",
    "  host=\"capstone-database.czwmid1hzf1x.us-west-2.rds.amazonaws.com\",\n",
    "  user=\"capstone_user\",\n",
    "  password=\"Capstone22!\",\n",
    "  database=\"mysqldb\"\n",
    ")\n",
    "\n",
    "mycursor = mydb.cursor()\n",
    "\n",
    "# set params\n",
    "mg_id = 'mg_01'\n",
    "\n",
    "params = {\n",
    "    'mg_id':mg_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagged_data_pred(df, lags):\n",
    "    df = df\n",
    "    for i in range(1, lags):\n",
    "        df[\"demand_lag_{}\".format(i)] = df['demand'].shift(i)\n",
    "        df[\"temp_lag_{}\".format(i)] = df['temp'].shift(i)\n",
    "        df[\"humidity_lag_{}\".format(i)] = df['humidity'].shift(i)\n",
    "\n",
    "    df = pd.DataFrame(df.iloc[-1]).T\n",
    "    return df\n",
    "\n",
    "def ModelPredictions(model, X_pred, mg_id):\n",
    "    prediction = model.predict(X_pred.drop(['end'], axis=1))\n",
    "    results = pd.DataFrame({'end':X_pred.end,\n",
    "                        'id':mg_id,\n",
    "                        'demand':prediction.round(1)  \n",
    "                       })    \n",
    "    return results\n",
    "\n",
    "def XGBModelPredictions(model, X_pred, mg_id, time_index):\n",
    "    prediction = model.predict(X_pred)\n",
    "    results = pd.DataFrame({'end':time_index,\n",
    "                            'id':mg_id,\n",
    "                            'demand':prediction\n",
    "                           }) \n",
    "    return results\n",
    "\n",
    "def SVRModelPredictions(model, X_pred, mg_id, time_index):\n",
    "    prediction = model.predict(X_pred)\n",
    "    prediction = prediction.reshape((1,-1))\n",
    "    prediction = sc_y.inverse_transform(prediction)\n",
    "    results = pd.DataFrame({'end':time_index,\n",
    "                            'id':mg_id,\n",
    "                            'demand':prediction[0].round(1)\n",
    "                           }) \n",
    "    return results\n",
    "\n",
    "#########\n",
    "# 15 Min\n",
    "def inference_15m(mg_id, params=params):\n",
    "    tail = pd.read_sql('''SELECT * FROM microgrid_actuals_15m WHERE id = %(mg_id)s ORDER BY end DESC LIMIT 97''', \n",
    "                                  con=credentials, params=params)\n",
    "    # invert the data frame\n",
    "    tail = tail.iloc[::-1]\n",
    "    # select the lastest date in the actuals table\n",
    "    date = tail.iloc[-1]['end']\n",
    "    # return a fixed frequency DatetimeIndex; grab the lastest date \n",
    "    time_index = pd.date_range(date, periods=16, freq='min')[-1]\n",
    "    # fill in empty record with latest date\n",
    "    tail.loc[tail.shape[0]] = [time_index, mg_id, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "    tail['month'] = tail['end'].dt.strftime('%b')\n",
    "\n",
    "    lower_ma = [m.lower() for m in month_abbr]\n",
    "\n",
    "    # one-liner with Pandas\n",
    "    tail['month_int'] = tail['month'].str.lower().map(lambda m: lower_ma.index(m)).astype('Int8')\n",
    "    tail['day_of_week'] = tail['end'].dt.day_name()\n",
    "    tail['day_of_week_int'] = tail['end'].dt.day_of_week\n",
    "\n",
    "    date_range = pd.date_range(start='2019-01-01', end='2022-01-27')\n",
    "\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=date_range.min(), end=date_range.max())\n",
    "    tail['holiday'] = tail['end'].dt.date.astype('datetime64').isin(holidays)\n",
    "    tail[\"holiday_int\"] = tail[\"holiday\"].astype(int)\n",
    "\n",
    "    tail = tail[['end', 'id','demand', 'temp', 'humidity', 'month_int', 'day_of_week_int', 'holiday_int']].copy() \n",
    "\n",
    "    # transform records to lagged data format\n",
    "    pred = lagged_data_pred(tail, 97)\n",
    "    X_pred = pred.drop(['id','demand', 'temp', 'humidity'], axis=1)\n",
    "\n",
    "    # set predict value\n",
    "    results = ModelPredictions(lr_15m, X_pred, mg_id)\n",
    "    # write results to sql table\n",
    "    results.to_sql('microgrid_predictions_15m', con=credentials, if_exists='append', index=False)\n",
    "\n",
    "    # select the next time step to predict\n",
    "    actual  = pd.read_sql('''SELECT * FROM microgrid_test_15m WHERE id = %(mg_id)s ORDER BY end LIMIT 1''', \n",
    "                          con=credentials, params=params)\n",
    "    # write next actual from the test table to the actual table\n",
    "    actual.to_sql('microgrid_actuals_15m', con=credentials, if_exists='append', index=False)\n",
    "    # delete updated record from test table\n",
    "    sql = \"DELETE FROM microgrid_test_15m WHERE id = '%s' AND end = '%s'\" % (mg_id, str(actual.iloc[0][0]))\n",
    "    mycursor.execute(sql)\n",
    "    mydb.commit()\n",
    "    print(\"15 min inference complete\")\n",
    "\n",
    "#########\n",
    "# 30 Min\n",
    "def inference_30m(mg_id, params=params):\n",
    "    tail = pd.read_sql('''SELECT * FROM microgrid_actuals_30m WHERE id = %(mg_id)s ORDER BY end DESC LIMIT 10''', \n",
    "                                  con=credentials, params=params)\n",
    "    # invert the data frame\n",
    "    tail = tail.iloc[::-1]\n",
    "    # select the lastest date in the actuals table\n",
    "    date = tail.iloc[-1]['end']\n",
    "    # return a fixed frequency DatetimeIndex; grab the lastest date \n",
    "    time_index = pd.date_range(date, periods=31, freq='min')[-1]\n",
    "    # fill in empty record with latest date\n",
    "    tail.loc[tail.shape[0]] = [time_index, mg_id, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "    tail['month'] = tail['end'].dt.strftime('%b')\n",
    "\n",
    "    lower_ma = [m.lower() for m in month_abbr]\n",
    "\n",
    "    # one-liner with Pandas\n",
    "    tail['month_int'] = tail['month'].str.lower().map(lambda m: lower_ma.index(m)).astype('Int8')\n",
    "    tail['day_of_week'] = tail['end'].dt.day_name()\n",
    "    tail['day_of_week_int'] = tail['end'].dt.day_of_week\n",
    "\n",
    "    date_range = pd.date_range(start='2019-01-01', end='2022-01-27')\n",
    "\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=date_range.min(), end=date_range.max())\n",
    "    tail['holiday'] = tail['end'].dt.date.astype('datetime64').isin(holidays)\n",
    "    tail[\"holiday_int\"] = tail[\"holiday\"].astype(int)\n",
    "\n",
    "    tail = tail[['end', 'id','demand', 'temp', 'humidity', 'month_int', 'day_of_week_int', 'holiday_int']].copy() \n",
    "\n",
    "    # transform records to lagged data format\n",
    "    pred = lagged_data_pred(tail, 10)\n",
    "    X_pred = pred.drop(['id','demand', 'temp', 'humidity'], axis=1)\n",
    "\n",
    "    # set predict value\n",
    "    results = ModelPredictions(lr_30m, X_pred, mg_id)\n",
    "    # write results to sql table\n",
    "    results.to_sql('microgrid_predictions_30m', con=credentials, if_exists='append', index=False)\n",
    "\n",
    "    # select the next time step to predict\n",
    "    actual  = pd.read_sql('''SELECT * FROM microgrid_test_30m WHERE id = %(mg_id)s ORDER BY end LIMIT 1''', \n",
    "                          con=credentials, params=params)\n",
    "    # write next actual from the test table to the actual table\n",
    "    actual.to_sql('microgrid_actuals_30m', con=credentials, if_exists='append', index=False)\n",
    "    # delete updated record from test table\n",
    "    sql = \"DELETE FROM microgrid_test_30m WHERE id = '%s' AND end = '%s'\" % (mg_id, str(actual.iloc[0][0]))\n",
    "    mycursor.execute(sql)\n",
    "    mydb.commit()\n",
    "    print(\"30 min inference complete\")\n",
    "\n",
    "#########\n",
    "# 45 Min\n",
    "def inference_45m(mg_id, params=params):\n",
    "    tail = pd.read_sql('''SELECT * FROM microgrid_actuals_45m WHERE id = %(mg_id)s ORDER BY end DESC LIMIT 10''', \n",
    "                                  con=credentials, params=params)\n",
    "    # invert the data frame\n",
    "    tail = tail.iloc[::-1]\n",
    "    # select the lastest date in the actuals table\n",
    "    date = tail.iloc[-1]['end']\n",
    "    # return a fixed frequency DatetimeIndex; grab the lastest date \n",
    "    time_index = pd.date_range(date, periods=46, freq='min')[-1]\n",
    "    # fill in empty record with latest date\n",
    "    tail.loc[tail.shape[0]] = [time_index, mg_id, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "    tail['month'] = tail['end'].dt.strftime('%b')\n",
    "\n",
    "    lower_ma = [m.lower() for m in month_abbr]\n",
    "\n",
    "    # one-liner with Pandas\n",
    "    tail['month_int'] = tail['month'].str.lower().map(lambda m: lower_ma.index(m)).astype('Int8')\n",
    "    tail['day_of_week'] = tail['end'].dt.day_name()\n",
    "    tail['day_of_week_int'] = tail['end'].dt.day_of_week\n",
    "\n",
    "    date_range = pd.date_range(start='2019-01-01', end='2022-01-27')\n",
    "\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=date_range.min(), end=date_range.max())\n",
    "    tail['holiday'] = tail['end'].dt.date.astype('datetime64').isin(holidays)\n",
    "    tail[\"holiday_int\"] = tail[\"holiday\"].astype(int)\n",
    "\n",
    "    tail = tail[['end', 'id','demand', 'temp', 'humidity', 'month_int', 'day_of_week_int', 'holiday_int']].copy() \n",
    "\n",
    "    # transform records to lagged data format\n",
    "    pred = lagged_data_pred(tail, 10)\n",
    "    X_pred = pred.drop(['id','demand', 'temp', 'humidity'], axis=1)\n",
    "\n",
    "    # set predict value\n",
    "    results = ModelPredictions(lr_45m, X_pred, mg_id)\n",
    "    # write results to sql table\n",
    "    results.to_sql('microgrid_predictions_45m', con=credentials, if_exists='append', index=False)\n",
    "\n",
    "    # select the next time step to predict\n",
    "    actual  = pd.read_sql('''SELECT * FROM microgrid_test_45m WHERE id = %(mg_id)s ORDER BY end LIMIT 1''', \n",
    "                          con=credentials, params=params)\n",
    "    # write next actual from the test table to the actual table\n",
    "    actual.to_sql('microgrid_actuals_45m', con=credentials, if_exists='append', index=False)\n",
    "    # delete updated record from test table\n",
    "    sql = \"DELETE FROM microgrid_test_45m WHERE id = '%s' AND end = '%s'\" % (mg_id, str(actual.iloc[0][0]))\n",
    "    mycursor.execute(sql)\n",
    "    mydb.commit()\n",
    "    print(\"45 min inference complete\")\n",
    "\n",
    "\n",
    "#########\n",
    "# 60 Min\n",
    "def inference_60m(mg_id, params=params):\n",
    "    tail = pd.read_sql('''SELECT * FROM microgrid_actuals_60m WHERE id = %(mg_id)s ORDER BY end DESC LIMIT 10''', \n",
    "                                  con=credentials, params=params)\n",
    "    # invert the data frame\n",
    "    tail = tail.iloc[::-1]\n",
    "    # select the lastest date in the actuals table\n",
    "    date = tail.iloc[-1]['end']\n",
    "    # return a fixed frequency DatetimeIndex; grab the lastest date \n",
    "    time_index = pd.date_range(date, periods=61, freq='min')[-1]\n",
    "    # fill in empty record with latest date\n",
    "    tail.loc[tail.shape[0]] = [time_index, mg_id, np.nan, np.nan, np.nan, np.nan]\n",
    "\n",
    "    tail['month'] = tail['end'].dt.strftime('%b')\n",
    "\n",
    "    lower_ma = [m.lower() for m in month_abbr]\n",
    "\n",
    "    # one-liner with Pandas\n",
    "    tail['month_int'] = tail['month'].str.lower().map(lambda m: lower_ma.index(m)).astype('Int8')\n",
    "    tail['day_of_week'] = tail['end'].dt.day_name()\n",
    "    tail['day_of_week_int'] = tail['end'].dt.day_of_week\n",
    "\n",
    "    date_range = pd.date_range(start='2019-01-01', end='2022-01-27')\n",
    "\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=date_range.min(), end=date_range.max())\n",
    "    tail['holiday'] = tail['end'].dt.date.astype('datetime64').isin(holidays)\n",
    "    tail[\"holiday_int\"] = tail[\"holiday\"].astype(int)\n",
    "\n",
    "    tail = tail[['end', 'id','demand', 'temp', 'humidity', 'month_int', 'day_of_week_int', 'holiday_int']].copy() \n",
    "\n",
    "    # transform records to lagged data format\n",
    "    pred = lagged_data_pred(tail, 10)\n",
    "    X_pred = pred.drop(['id','demand', 'temp', 'humidity'], axis=1)\n",
    "\n",
    "    # set predict value\n",
    "    results = ModelPredictions(lr_60m, X_pred, mg_id)\n",
    "    # write results to sql table\n",
    "    results.to_sql('microgrid_predictions_60m', con=credentials, if_exists='append', index=False)\n",
    "\n",
    "    # select the next time step to predict\n",
    "    actual  = pd.read_sql('''SELECT * FROM microgrid_test_60m WHERE id = %(mg_id)s ORDER BY end LIMIT 1''', \n",
    "                          con=credentials, params=params)\n",
    "    # write next actual from the test table to the actual table\n",
    "    actual.to_sql('microgrid_actuals_60m', con=credentials, if_exists='append', index=False)\n",
    "    # delete updated record from test table\n",
    "    sql = \"DELETE FROM microgrid_test_60m WHERE id = '%s' AND end = '%s'\" % (mg_id, str(actual.iloc[0][0]))\n",
    "    mycursor.execute(sql)\n",
    "    mydb.commit()\n",
    "    print(\"60 min inference complete\")\n",
    "\n",
    "#########\n",
    "# 24 Hour\n",
    "def inference_hr_24(mg_id, params=params):\n",
    "    tail = pd.read_sql('''SELECT * FROM microgrid_actuals_hr_24 WHERE id = %(mg_id)s ORDER BY end DESC LIMIT 10''', \n",
    "                                              con=credentials, params=params)\n",
    "    # invert the data frame\n",
    "    tail = tail.iloc[::-1]\n",
    "    # select the lastest date in the actuals table\n",
    "    date = tail.iloc[-1]['end']\n",
    "    # return a fixed frequency DatetimeIndex; grab the lastest date \n",
    "    time_index = pd.date_range(date, periods=25, freq='h')[-1]\n",
    "    # fill in empty record with latest date\n",
    "    tail.loc[tail.shape[0]] = [time_index, mg_id, np.nan, np.nan, np.nan, np.nan]\n",
    "    # set prediction month\n",
    "    tail['month'] = tail['end'].dt.strftime('%b')\n",
    "    lower_ma = [m.lower() for m in month_abbr]\n",
    "    tail['month_int'] = tail['month'].str.lower().map(lambda m: lower_ma.index(m)).astype('Int8')\n",
    "    tail['day_of_week'] = tail['end'].dt.day_name()\n",
    "    tail['day_of_week_int'] = tail['end'].dt.day_of_week\n",
    "    date_range = pd.date_range(start='2019-01-01', end='2022-01-27')\n",
    "    cal = calendar()\n",
    "    holidays = cal.holidays(start=date_range.min(), end=date_range.max())\n",
    "    tail['holiday'] = tail['end'].dt.date.astype('datetime64').isin(holidays)\n",
    "    tail[\"holiday_int\"] = tail[\"holiday\"].astype(int)\n",
    "    tail = tail[['end', 'id',  'month_int', 'day_of_week_int', 'holiday_int', 'demand', 'temp', 'humidity']].copy()\n",
    "\n",
    "    # lag data\n",
    "    lr_pred = lagged_data_pred(tail, 10)\n",
    "    rf_pred = lagged_data_pred(tail, 10)\n",
    "    xgb_pred = lagged_data_pred(tail, 10)\n",
    "    knn_pred = lagged_data_pred(tail, 10)\n",
    "    svr_pred = lagged_data_pred(tail, 10)\n",
    "\n",
    "    # prepare model X pred\n",
    "    lr_X_pred = lr_pred.drop(['id','demand', 'temp', 'humidity'], axis=1)\n",
    "    rf_X_pred = rf_pred.drop(['id','demand', 'temp', 'humidity'], axis=1)\n",
    "    xgb_X_pred = xgb_pred.drop(['id','demand', 'temp', 'humidity'], axis=1)\n",
    "    xgb_X_pred = xgb_X_pred.loc[:, xgb_X_pred.columns != 'end'].astype(float, errors = 'raise')\n",
    "    xgb_X_pred['month_int'] = lbl.transform(xgb_pred['month_int'].astype(str))\n",
    "    knn_X_pred = knn_pred.drop(['id','demand', 'temp', 'humidity'], axis=1)\n",
    "    svr_X_pred = svr_pred.drop(['end','id','demand', 'temp', 'humidity'], axis=1)\n",
    "    svr_X_pred_scaled = sc_X.transform(svr_X_pred)\n",
    "\n",
    "    # prediction results\n",
    "    lr_results = ModelPredictions(lr_model, lr_X_pred, mg_id).set_index('end').rename(columns={\"demand\": \"lr_demand\"}) \n",
    "    rf_results = ModelPredictions(rf_model, rf_X_pred, mg_id).set_index('end').rename(columns={\"demand\": \"rf_demand\"}) \n",
    "    xgb_results = XGBModelPredictions(xgb_model, xgb_X_pred, mg_id, tail['end'].iloc[-1]).set_index('end').rename(columns={\"demand\": \"xgb_demand\"})\n",
    "    knn_results = ModelPredictions(knn_model, knn_X_pred, mg_id).set_index('end').rename(columns={\"demand\": \"knn_demand\"}) \n",
    "    svr_results = SVRModelPredictions(svr_model, svr_X_pred_scaled, mg_id, tail['end'].iloc[-1]).set_index('end').rename(columns={\"demand\": \"svr_demand\"}) \n",
    "\n",
    "    # # combine data sets\n",
    "    frames = [lr_results.lr_demand, rf_results.rf_demand, xgb_results.xgb_demand, knn_results.knn_demand, svr_results.svr_demand]\n",
    "    ensemble_X_pred = pd.concat(frames, axis=1, join=\"inner\")\n",
    "    ensemble_X_pred['month_int'] = tail.iloc[-1].month_int\n",
    "    ensemble_X_pred['day_of_week_int'] = tail.iloc[-1].day_of_week_int\n",
    "    ensemble_X_pred['holiday_int'] = tail.iloc[-1].holiday_int\n",
    "\n",
    "    # ensemble pred result\n",
    "    ensemble_X_pred.reset_index(inplace=True)\n",
    "    ensemble_results = ModelPredictions(ensemble_model, ensemble_X_pred, mg_id)\n",
    "    ensemble_results.to_sql('microgrid_predictions_hr_24', con=credentials, if_exists='append', index=False)\n",
    "\n",
    "    # time.sleep(5)\n",
    "    # select the next time step to predict\n",
    "    actual  = pd.read_sql('''SELECT * FROM microgrid_test_hr_24 WHERE id = %(mg_id)s ORDER BY end LIMIT 1''', \n",
    "                          con=credentials, params=params)\n",
    "    # write next actual from the test table to the actual table\n",
    "    actual.to_sql('microgrid_actuals_hr_24', con=credentials, if_exists='append', index=False)\n",
    "    # delete updated record from test table\n",
    "    sql = \"DELETE FROM microgrid_test_hr_24 WHERE id = '%s' AND end = '%s'\" % (mg_id, str(actual.iloc[0][0]))\n",
    "    mycursor.execute(sql)\n",
    "    mydb.commit()\n",
    "    print(\"24 hour inference complete\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "60 min inference complete\n",
      "15 min inference complete\n",
      "45 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "15 min inference complete\n",
      "15 min inference complete\n",
      "30 min inference complete\n",
      "45 min inference complete\n",
      "60 min inference complete\n",
      "done\n",
      "506.0939221382141\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "i = 0\n",
    "while i < 5:\n",
    "    # 15 min\n",
    "    inference_15m(mg_id)\n",
    "    # 30 min\n",
    "    inference_15m(mg_id)\n",
    "    inference_30m(mg_id)\n",
    "    # 45 min\n",
    "    inference_15m(mg_id)\n",
    "    inference_45m(mg_id)\n",
    "    # 60 min\n",
    "    inference_15m(mg_id)\n",
    "    inference_30m(mg_id)\n",
    "    inference_60m(mg_id)\n",
    "    \n",
    "    # 15 min\n",
    "    inference_15m(mg_id)\n",
    "    # 30 min\n",
    "    inference_15m(mg_id)\n",
    "    inference_30m(mg_id)\n",
    "    inference_45m(mg_id)\n",
    "    # 45 min\n",
    "    inference_15m(mg_id)\n",
    "    # 60 min\n",
    "    inference_15m(mg_id)\n",
    "    inference_30m(mg_id)\n",
    "    inference_60m(mg_id)\n",
    "    \n",
    "    # 15 min\n",
    "    inference_15m(mg_id)\n",
    "    inference_45m(mg_id)\n",
    "    # 30 min\n",
    "    inference_15m(mg_id)\n",
    "    inference_30m(mg_id)\n",
    "    # 45 min\n",
    "    inference_15m(mg_id)\n",
    "    # 60 min\n",
    "    inference_15m(mg_id)\n",
    "    inference_30m(mg_id)\n",
    "    inference_45m(mg_id)\n",
    "    inference_60m(mg_id)\n",
    "    \n",
    "    i+=1 \n",
    "print(\"done\")\n",
    "stop = time.time()\n",
    "timer = stop - start\n",
    "print(timer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
